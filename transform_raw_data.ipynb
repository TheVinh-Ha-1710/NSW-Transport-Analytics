{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Transform raw datasets with AWS Glue Notebook Interactive Session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "####  Set up and start the interactive session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Glue Interactive Sessions Kernel\n",
      "For more information on available magic commands, please type %help in any new cell.\n",
      "\n",
      "Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
      "The following configurations have been updated: {'region': 'ap-southeast-2', 'iam_role': 'arn:aws:iam::877030647703:role/nsw-transport-analytics', 'idle_timeout': 2880, 'glue_version': '5.0', 'worker_type': 'G.1X', 'number_of_workers': 5}\n"
     ]
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    'region': 'ap-southeast-2',\n",
    "    'iam_role': 'arn:aws:iam::877030647703:role/nsw-transport-analytics',\n",
    "    'idle_timeout' : 2880,\n",
    "    'glue_version': '5.0',\n",
    "    'worker_type': 'G.1X',\n",
    "    'number_of_workers': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create a Glue session for the kernel.\n",
      "Session Type: etl\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 5\n",
      "Idle Timeout: 2880\n",
      "Session ID: 49875323-03be-424d-a320-b4ce26c625b6\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 1.0.9\n",
      "--enable-glue-datacatalog true\n",
      "Waiting for session 49875323-03be-424d-a320-b4ce26c625b6 to get into ready status...\n",
      "Session 49875323-03be-424d-a320-b4ce26c625b6 has been created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create a DynamicFrame for raw data tables from AWS Glue Data Catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- year_month: string\n",
      "|-- card type: string\n",
      "|-- travel_mode: string\n",
      "|-- trip: double\n"
     ]
    }
   ],
   "source": [
    "# Create DynamicFrame for raw dataset\n",
    "raw = glueContext.create_dynamic_frame.from_catalog(database='nsw-transport-raw', table_name='all_modes_csv')\n",
    "raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- year_month: string\n",
      "|-- travel_mode: string\n",
      "|-- trip: double\n",
      "|-- card_type: string\n"
     ]
    }
   ],
   "source": [
    "# Rename the columns of the raw dataset\n",
    "raw = raw.rename_field('card type','card_type')\n",
    "raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- col0: string\n",
      "|-- col1: string\n",
      "|-- col2: string\n",
      "|-- col3: string\n"
     ]
    }
   ],
   "source": [
    "# # Create DynamicFrame for legacy dataset\n",
    "raw_legacy = glueContext.create_dynamic_frame.from_catalog(database='nsw-transport-raw', table_name='all_modes_legacy_csv')\n",
    "raw_legacy.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- year_month: string\n",
      "|-- card_type: string\n",
      "|-- travel_mode: string\n",
      "|-- trip: string\n"
     ]
    }
   ],
   "source": [
    "# Rename the columns of the legacy dataset\n",
    "raw_legacy = raw_legacy.rename_field('col0','year_month')\n",
    "raw_legacy = raw_legacy.rename_field('col1','card_type')\n",
    "raw_legacy = raw_legacy.rename_field('col2','travel_mode')\n",
    "raw_legacy = raw_legacy.rename_field('col3','trip')\n",
    "raw_legacy.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Convert the DynamicFrame to a Spark DataFrame for transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+---------+\n",
      "|year_month|travel_mode|     trip|card_type|\n",
      "+----------+-----------+---------+---------+\n",
      "|   2024-01|      Ferry| 504780.0|    Adult|\n",
      "|   2024-01| Light Rail|1203585.0|    Adult|\n",
      "|   2024-01|      Metro| 709103.0|    Adult|\n",
      "|   2024-01|      Train|9987934.0|    Adult|\n",
      "|   2024-01|unallocated|      5.0|    Adult|\n",
      "+----------+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n"
     ]
    }
   ],
   "source": [
    "# Convert to Spark DataFrame for raw dataset\n",
    "df_raw = raw.toDF()\n",
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-------+\n",
      "|year_month|  card_type|travel_mode|   trip|\n",
      "+----------+-----------+-----------+-------+\n",
      "|Year_Month|  Card Type|Travel_Mode|   Trip|\n",
      "|    Feb-23|      Adult|        Bus|6556480|\n",
      "|    Jan-23|   Employee|        Bus|  53044|\n",
      "|    Dec-22|Child/Youth|        Bus| 779633|\n",
      "|    Dec-22|      Adult|        Bus|6005060|\n",
      "+----------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Convert to Spark DataFrame for legacy dataset\n",
    "df_legacy = raw_legacy.toDF()\n",
    "df_legacy.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define reusable transformation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to exclude the misread header as first row in DataFrame\n",
    "def exclude_header(df):\n",
    "    excluded_header_df = df.withColumn(\"row_num\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))) \\\n",
    "       .filter(\"row_num > 1\") \\\n",
    "       .drop(\"row_num\")\n",
    "    return excluded_header_df\n",
    "\n",
    "# Function to cast a single column\n",
    "def cast_column_dtype(df, col_name, dtype):\n",
    "    return df.withColumn(col_name, F.col(col_name).cast(dtype))\n",
    "\n",
    "# Function to cast multiple columns\n",
    "def cast_df(df, col_list, dtype_list):\n",
    "    df_casted = df  # start from original df\n",
    "    for col, dtype in zip(col_list, dtype_list):\n",
    "        df_casted = cast_column_dtype(df_casted, col, dtype)\n",
    "    return df_casted\n",
    "\n",
    "# Function to format year month\n",
    "def format_year_month(df, col):\n",
    "    df_reformated = df.withColumn(\n",
    "        col,\n",
    "        F.date_format(\n",
    "            F.to_date(F.concat(F.col(col), F.lit('-01')), 'yyyy-MM-dd'),\n",
    "            'MMM-yyyy'\n",
    "        )\n",
    "    )\n",
    "    return df_reformated\n",
    "\n",
    "# Function to format short year to full length year\n",
    "def expand_year_month(df, col):\n",
    "    df_reformated = df.withColumn(\n",
    "        col,\n",
    "        F.date_format(\n",
    "            F.to_date(F.col(col), \"MMM-yy\"),\n",
    "            \"MMM-yyyy\"\n",
    "        )\n",
    "    )\n",
    "    return df_reformated\n",
    "\n",
    "# Function to add separated year and month columns\n",
    "def add_date_columns(df):\n",
    "    df_added = df.withColumn(\n",
    "        \"year_num\", \n",
    "        F.date_format(F.to_date(F.col(\"year_month\"), \"MMM-yyyy\"), \"yyyy\")\n",
    "    ).withColumn(\n",
    "        \"month_num\",\n",
    "        F.date_format(F.to_date(F.col(\"year_month\"), \"MMM-yyyy\"), \"MM\")\n",
    "    )\n",
    "    return df_added\n",
    "\n",
    "# Function to union DataFrames\n",
    "def union_df(df1, df2):\n",
    "    full_df = df1.unionByName(df2, allowMissingColumns = False)\n",
    "    return full_df\n",
    "\n",
    "# Function to check and remove duplicates\n",
    "def check_remove_duplicates(df):\n",
    "    df_no_dups = df.dropDuplicates()\n",
    "    count_before = df.count()\n",
    "    count_after = df_no_dups.count()\n",
    "    print(f\"Count before: {count_before}\")\n",
    "    print(f\"Count after: {count_after}\")\n",
    "    print(f\"Total duplicates removed: {count_before - count_after}\")\n",
    "    return df_no_dups\n",
    "\n",
    "# Function to check and remove rows with null values\n",
    "def check_remove_nulls(df):\n",
    "    null_counts = df.select([  # Count nulls per column\n",
    "        F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "    print(\"Null counts per column:\")\n",
    "    null_counts.show() \n",
    "    df_no_nulls = df.na.drop() # Remove rows with any nulls\n",
    "    print(f\"Count before: {df.count()}\")\n",
    "    print(f\"Count after: {df_no_nulls.count()}\")\n",
    "    print(f\"Total rows removed: {df.count() - df_no_nulls.count()}\")\n",
    "    return df_no_nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform raw datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-------+\n",
      "|year_month|  card_type|travel_mode|   trip|\n",
      "+----------+-----------+-----------+-------+\n",
      "|    Feb-23|      Adult|        Bus|6556480|\n",
      "|    Jan-23|   Employee|        Bus|  53044|\n",
      "|    Dec-22|Child/Youth|        Bus| 779633|\n",
      "|    Dec-22|      Adult|        Bus|6005060|\n",
      "|    Nov-22|Free Travel|        Bus|  29299|\n",
      "+----------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Remove the misread header from legacy dataset\n",
    "legacy_cleaned = exclude_header(df_legacy)\n",
    "legacy_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-------+\n",
      "|year_month|  card_type|travel_mode|   trip|\n",
      "+----------+-----------+-----------+-------+\n",
      "|  Feb-2023|      Adult|        Bus|6556480|\n",
      "|  Jan-2023|   Employee|        Bus|  53044|\n",
      "|  Dec-2022|Child/Youth|        Bus| 779633|\n",
      "|  Dec-2022|      Adult|        Bus|6005060|\n",
      "|  Nov-2022|Free Travel|        Bus|  29299|\n",
      "+----------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Expand the year to full length on legacy dataset\n",
    "legacy_formated = expand_year_month(legacy_cleaned, 'year_month')\n",
    "legacy_formated.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+---------+\n",
      "|year_month|travel_mode|     trip|card_type|\n",
      "+----------+-----------+---------+---------+\n",
      "|  Jan-2024|      Ferry| 504780.0|    Adult|\n",
      "|  Jan-2024| Light Rail|1203585.0|    Adult|\n",
      "|  Jan-2024|      Metro| 709103.0|    Adult|\n",
      "|  Jan-2024|      Train|9987934.0|    Adult|\n",
      "|  Jan-2024|unallocated|      5.0|    Adult|\n",
      "+----------+-----------+---------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Reformat month_year column of raw dataset\n",
    "raw_cleaned = format_year_month(df_raw, 'year_month')\n",
    "raw_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+-------+\n",
      "|year_month|           card_type|travel_mode|   trip|\n",
      "+----------+--------------------+-----------+-------+\n",
      "|  Feb-2023|               Adult|        Bus|6556480|\n",
      "|  Jan-2023|            Employee|        Bus|  53044|\n",
      "|  Dec-2022|         Child/Youth|        Bus| 779633|\n",
      "|  Dec-2022|               Adult|        Bus|6005060|\n",
      "|  Nov-2022|         Free Travel|        Bus|  29299|\n",
      "|  Jun-2022|            Employee|        Bus|  55746|\n",
      "|  Dec-2021|Adult Single Bus ...|        Bus|    124|\n",
      "|  Mar-2021|            Employee|        Bus|  62780|\n",
      "|  Jul-2020|         Child/Youth|        Bus| 863848|\n",
      "|  Mar-2020|               Adult|        Bus|9373131|\n",
      "|  Feb-2020|         Free Travel|        Bus|  39465|\n",
      "|  Oct-2019|Adult Single Bus ...|        Bus|  68763|\n",
      "|  Sep-2019|Child/Youth Singl...|        Bus|   3586|\n",
      "|  Feb-2019|Adult Single Bus ...|        Bus|  54290|\n",
      "|  Mar-2018|Adult Single Bus ...|        Bus| 149812|\n",
      "|  Oct-2017|Adult Single Bus ...|        Bus| 154191|\n",
      "|  Sep-2016|Child/Youth Singl...|        Bus|  14739|\n",
      "|  Sep-2024|                 CTP|        Bus|7093100|\n",
      "|  Jul-2022|          Concession|        Bus| 619503|\n",
      "|  Jan-2022|         Free Travel|        Bus|  16557|\n",
      "+----------+--------------------+-----------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Union legacy and raw dataset to make full dataset \n",
    "df_union = union_df(legacy_formated, raw_cleaned)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-------+--------+---------+\n",
      "|year_month|  card_type|travel_mode|   trip|year_num|month_num|\n",
      "+----------+-----------+-----------+-------+--------+---------+\n",
      "|  Feb-2023|      Adult|        Bus|6556480|    2023|       02|\n",
      "|  Jan-2023|   Employee|        Bus|  53044|    2023|       01|\n",
      "|  Dec-2022|Child/Youth|        Bus| 779633|    2022|       12|\n",
      "|  Dec-2022|      Adult|        Bus|6005060|    2022|       12|\n",
      "|  Nov-2022|Free Travel|        Bus|  29299|    2022|       11|\n",
      "+----------+-----------+-----------+-------+--------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_extra = add_date_columns(df_union)\n",
    "df_extra.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year_month: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- travel_mode: string (nullable = true)\n",
      " |-- trip: long (nullable = true)\n",
      " |-- year_num: integer (nullable = true)\n",
      " |-- month_num: integer (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "# Cast correct data type to numeric columns\n",
    "col_list = ['trip', 'year_num', 'month_num']\n",
    "dtype_list = [LongType(), IntegerType(), IntegerType()]\n",
    "df_casted = cast_df(df_extra, col_list, dtype_list)\n",
    "df_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count before: 6551\n",
      "Count after: 6542\n",
      "Total duplicates removed: 9\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates from full data\n",
    "df_no_dups = check_remove_duplicates(df_casted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts per column:\n",
      "+----------+---------+-----------+----+--------+---------+\n",
      "|year_month|card_type|travel_mode|trip|year_num|month_num|\n",
      "+----------+---------+-----------+----+--------+---------+\n",
      "|         0|        0|          0|  24|       0|        0|\n",
      "+----------+---------+-----------+----+--------+---------+\n",
      "\n",
      "Count before: 6542\n",
      "Count after: 6518\n",
      "Total rows removed: 24\n"
     ]
    }
   ],
   "source": [
    "# Check and remove null values\n",
    "df_cleaned = check_remove_nulls(df_no_dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+-----------+----------+\n",
      "|month_year|month_num|year_num|           card_type|travel_mode|trip_count|\n",
      "+----------+---------+--------+--------------------+-----------+----------+\n",
      "|  Aug-2019|        8|    2019|         Free Travel|        Bus|     42665|\n",
      "|  May-2022|        5|    2022|         Child/Youth|        Bus|    550795|\n",
      "|  Feb-2017|        2|    2017|Adult Single Bus ...|        Bus|     39559|\n",
      "|  Jan-2019|        1|    2019|    Senior/Pensioner|        Bus|   3970989|\n",
      "|  May-2019|        5|    2019|    Senior/Pensioner|        Bus|   4586037|\n",
      "+----------+---------+--------+--------------------+-----------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Reorder, rename neccessary columns and finalise dataset\n",
    "new_order = ['year_month', 'month_num', 'year_num', 'card_type', 'travel_mode', 'trip']\n",
    "df_final = df_cleaned.select(new_order) # Reorder columns\n",
    "df_final = (  # Rename columns\n",
    "    df_final\n",
    "    .withColumnRenamed('year_month', 'month_year')\n",
    "    .withColumnRenamed('trip', 'trip_count')\n",
    ")\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "tags": []
   },
   "source": [
    "#### Write transformed data in the DynamicFrame to S3 and update AWS Glue Data Catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- month_year: string\n",
      "|-- month_num: int\n",
      "|-- year_num: int\n",
      "|-- card_type: string\n",
      "|-- travel_mode: string\n",
      "|-- trip_count: long\n"
     ]
    }
   ],
   "source": [
    "# Make a DyF for final dataset\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "DyF = DynamicFrame.fromDF(df_final, glueContext, 'transformed_dataset')\n",
    "DyF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<awsglue.dynamicframe.DynamicFrame object at 0x7f39f541ead0>\n"
     ]
    }
   ],
   "source": [
    "# Write DyF to S3 bucket as parquet format\n",
    "s3output = glueContext.getSink(\n",
    "  path=\"s3://nsw-transport-data/transformed\",\n",
    "  connection_type=\"s3\",\n",
    "  partitionKeys=['year_num', 'month_num'],\n",
    "  compression=\"snappy\",\n",
    "  transformation_ctx=\"s3output\",\n",
    ")\n",
    "s3output.setFormat(\"glueparquet\")\n",
    "s3output.writeFrame(DyF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
